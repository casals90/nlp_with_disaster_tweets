{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --upgrade pandas-profiling\n!pip install --upgrade hypertools\n!pip install --upgrade pandas","execution_count":1,"outputs":[{"output_type":"stream","text":"Collecting pandas-profiling\n  Downloading pandas_profiling-2.6.0-py2.py3-none-any.whl (241 kB)\n\u001b[K     |████████████████████████████████| 241 kB 2.7 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied, skipping upgrade: jinja2>=2.11.1 in /opt/conda/lib/python3.7/site-packages (from pandas-profiling) (2.11.1)\nRequirement already satisfied, skipping upgrade: missingno>=0.4.2 in /opt/conda/lib/python3.7/site-packages (from pandas-profiling) (0.4.2)\nRequirement already satisfied, skipping upgrade: ipywidgets>=7.5.1 in /opt/conda/lib/python3.7/site-packages (from pandas-profiling) (7.5.1)\nRequirement already satisfied, skipping upgrade: tqdm>=4.43.0 in /opt/conda/lib/python3.7/site-packages (from pandas-profiling) (4.43.0)\nRequirement already satisfied, skipping upgrade: numpy>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from pandas-profiling) (1.18.1)\nCollecting tangled-up-in-unicode>=0.0.4\n  Downloading tangled_up_in_unicode-0.0.4.tar.gz (1.5 MB)\n\u001b[K     |████████████████████████████████| 1.5 MB 5.1 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied, skipping upgrade: statsmodels>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from pandas-profiling) (0.11.1)\nRequirement already satisfied, skipping upgrade: confuse>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from pandas-profiling) (1.0.0)\nRequirement already satisfied, skipping upgrade: htmlmin>=0.1.12 in /opt/conda/lib/python3.7/site-packages (from pandas-profiling) (0.1.12)\nRequirement already satisfied, skipping upgrade: scipy>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from pandas-profiling) (1.4.1)\nCollecting phik>=0.9.10\n  Downloading phik-0.9.11-py3-none-any.whl (609 kB)\n\u001b[K     |████████████████████████████████| 609 kB 15.8 MB/s eta 0:00:01\n\u001b[?25hCollecting visions[type_image_path]>=0.4.1\n  Downloading visions-0.4.1-py3-none-any.whl (58 kB)\n\u001b[K     |████████████████████████████████| 58 kB 3.5 MB/s  eta 0:00:01\n\u001b[?25hRequirement already satisfied, skipping upgrade: pandas>=0.25.3 in /opt/conda/lib/python3.7/site-packages (from pandas-profiling) (1.0.1)\nRequirement already satisfied, skipping upgrade: requests>=2.23.0 in /opt/conda/lib/python3.7/site-packages (from pandas-profiling) (2.23.0)\nRequirement already satisfied, skipping upgrade: astropy>=4.0 in /opt/conda/lib/python3.7/site-packages (from pandas-profiling) (4.0)\nCollecting matplotlib>=3.2.0\n  Downloading matplotlib-3.2.1-cp37-cp37m-manylinux1_x86_64.whl (12.4 MB)\n\u001b[K     |████████████████████████████████| 12.4 MB 17.9 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /opt/conda/lib/python3.7/site-packages (from jinja2>=2.11.1->pandas-profiling) (1.1.1)\nRequirement already satisfied, skipping upgrade: seaborn in /opt/conda/lib/python3.7/site-packages (from missingno>=0.4.2->pandas-profiling) (0.10.0)\nRequirement already satisfied, skipping upgrade: ipykernel>=4.5.1 in /opt/conda/lib/python3.7/site-packages (from ipywidgets>=7.5.1->pandas-profiling) (5.1.1)\nRequirement already satisfied, skipping upgrade: nbformat>=4.2.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets>=7.5.1->pandas-profiling) (5.0.4)\nRequirement already satisfied, skipping upgrade: traitlets>=4.3.1 in /opt/conda/lib/python3.7/site-packages (from ipywidgets>=7.5.1->pandas-profiling) (4.3.3)\nRequirement already satisfied, skipping upgrade: ipython>=4.0.0; python_version >= \"3.3\" in /opt/conda/lib/python3.7/site-packages (from ipywidgets>=7.5.1->pandas-profiling) (7.12.0)\nRequirement already satisfied, skipping upgrade: widgetsnbextension~=3.5.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets>=7.5.1->pandas-profiling) (3.5.1)\nRequirement already satisfied, skipping upgrade: patsy>=0.5 in /opt/conda/lib/python3.7/site-packages (from statsmodels>=0.11.1->pandas-profiling) (0.5.1)\nRequirement already satisfied, skipping upgrade: pyyaml in /opt/conda/lib/python3.7/site-packages (from confuse>=1.0.0->pandas-profiling) (5.3)\nRequirement already satisfied, skipping upgrade: numba>=0.38.1 in /opt/conda/lib/python3.7/site-packages (from phik>=0.9.10->pandas-profiling) (0.48.0)\nRequirement already satisfied, skipping upgrade: joblib>=0.14.1 in /opt/conda/lib/python3.7/site-packages (from phik>=0.9.10->pandas-profiling) (0.14.1)\nRequirement already satisfied, skipping upgrade: attrs>=19.3.0 in /opt/conda/lib/python3.7/site-packages (from visions[type_image_path]>=0.4.1->pandas-profiling) (19.3.0)\nRequirement already satisfied, skipping upgrade: networkx>=2.4 in /opt/conda/lib/python3.7/site-packages (from visions[type_image_path]>=0.4.1->pandas-profiling) (2.4)\nRequirement already satisfied, skipping upgrade: imagehash; extra == \"type_image_path\" in /opt/conda/lib/python3.7/site-packages (from visions[type_image_path]>=0.4.1->pandas-profiling) (4.0)\nRequirement already satisfied, skipping upgrade: Pillow; extra == \"type_image_path\" in /opt/conda/lib/python3.7/site-packages (from visions[type_image_path]>=0.4.1->pandas-profiling) (5.4.1)\nRequirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.25.3->pandas-profiling) (2.8.1)\nRequirement already satisfied, skipping upgrade: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.25.3->pandas-profiling) (2019.3)\nRequirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.23.0->pandas-profiling) (3.0.4)\nRequirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.23.0->pandas-profiling) (2020.4.5.1)\nRequirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.23.0->pandas-profiling) (1.25.7)\nRequirement already satisfied, skipping upgrade: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.23.0->pandas-profiling) (2.9)\nRequirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.2.0->pandas-profiling) (2.4.6)\nRequirement already satisfied, skipping upgrade: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.2.0->pandas-profiling) (0.10.0)\nRequirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.2.0->pandas-profiling) (1.1.0)\nRequirement already satisfied, skipping upgrade: jupyter-client in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.1->pandas-profiling) (5.3.4)\nRequirement already satisfied, skipping upgrade: tornado>=4.2 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.1->pandas-profiling) (5.0.2)\nRequirement already satisfied, skipping upgrade: jupyter-core in /opt/conda/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets>=7.5.1->pandas-profiling) (4.6.3)\nRequirement already satisfied, skipping upgrade: ipython-genutils in /opt/conda/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets>=7.5.1->pandas-profiling) (0.2.0)\nRequirement already satisfied, skipping upgrade: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets>=7.5.1->pandas-profiling) (3.2.0)\nRequirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.7/site-packages (from traitlets>=4.3.1->ipywidgets>=7.5.1->pandas-profiling) (1.14.0)\nRequirement already satisfied, skipping upgrade: decorator in /opt/conda/lib/python3.7/site-packages (from traitlets>=4.3.1->ipywidgets>=7.5.1->pandas-profiling) (4.4.1)\nRequirement already satisfied, skipping upgrade: jedi>=0.10 in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->pandas-profiling) (0.14.1)\nRequirement already satisfied, skipping upgrade: pickleshare in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->pandas-profiling) (0.7.5)\nRequirement already satisfied, skipping upgrade: pexpect; sys_platform != \"win32\" in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->pandas-profiling) (4.8.0)\nRequirement already satisfied, skipping upgrade: backcall in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->pandas-profiling) (0.1.0)\nRequirement already satisfied, skipping upgrade: pygments in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->pandas-profiling) (2.5.2)\nRequirement already satisfied, skipping upgrade: setuptools>=18.5 in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->pandas-profiling) (45.2.0.post20200209)\n","name":"stdout"},{"output_type":"stream","text":"Requirement already satisfied, skipping upgrade: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->pandas-profiling) (2.0.10)\nRequirement already satisfied, skipping upgrade: notebook>=4.4.1 in /opt/conda/lib/python3.7/site-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas-profiling) (5.5.0)\nRequirement already satisfied, skipping upgrade: llvmlite<0.32.0,>=0.31.0dev0 in /opt/conda/lib/python3.7/site-packages (from numba>=0.38.1->phik>=0.9.10->pandas-profiling) (0.31.0)\nRequirement already satisfied, skipping upgrade: pywavelets in /opt/conda/lib/python3.7/site-packages (from imagehash; extra == \"type_image_path\"->visions[type_image_path]>=0.4.1->pandas-profiling) (1.1.1)\nRequirement already satisfied, skipping upgrade: pyzmq>=13 in /opt/conda/lib/python3.7/site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.5.1->pandas-profiling) (18.1.1)\nRequirement already satisfied, skipping upgrade: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>=7.5.1->pandas-profiling) (0.15.7)\nRequirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>=7.5.1->pandas-profiling) (1.5.0)\nRequirement already satisfied, skipping upgrade: parso>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from jedi>=0.10->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->pandas-profiling) (0.6.1)\nRequirement already satisfied, skipping upgrade: ptyprocess>=0.5 in /opt/conda/lib/python3.7/site-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->pandas-profiling) (0.6.0)\nRequirement already satisfied, skipping upgrade: wcwidth in /opt/conda/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.1->pandas-profiling) (0.1.8)\nRequirement already satisfied, skipping upgrade: terminado>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas-profiling) (0.8.3)\nRequirement already satisfied, skipping upgrade: Send2Trash in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas-profiling) (1.5.0)\nRequirement already satisfied, skipping upgrade: nbconvert in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas-profiling) (5.6.1)\nRequirement already satisfied, skipping upgrade: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>=7.5.1->pandas-profiling) (3.0.0)\nRequirement already satisfied, skipping upgrade: mistune<2,>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas-profiling) (0.8.4)\nRequirement already satisfied, skipping upgrade: entrypoints>=0.2.2 in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas-profiling) (0.3)\nRequirement already satisfied, skipping upgrade: bleach in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas-profiling) (3.1.1)\nRequirement already satisfied, skipping upgrade: pandocfilters>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas-profiling) (1.4.2)\nRequirement already satisfied, skipping upgrade: defusedxml in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas-profiling) (0.6.0)\nRequirement already satisfied, skipping upgrade: testpath in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas-profiling) (0.4.4)\nRequirement already satisfied, skipping upgrade: webencodings in /opt/conda/lib/python3.7/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.1->pandas-profiling) (0.5.1)\nBuilding wheels for collected packages: tangled-up-in-unicode\n  Building wheel for tangled-up-in-unicode (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for tangled-up-in-unicode: filename=tangled_up_in_unicode-0.0.4-py3-none-any.whl size=1545721 sha256=e74f0445343fc9dcab8269f2ace66c9d43b1e9fca1014a96c00bdbaad6328581\n  Stored in directory: /root/.cache/pip/wheels/22/bb/74/1c79b7768407fd0acee5a61216474bd95b05ccdd38eac9c992\nSuccessfully built tangled-up-in-unicode\n\u001b[31mERROR: hypertools 0.6.2 has requirement scikit-learn<0.22,>=0.19.1, but you'll have scikit-learn 0.22.2.post1 which is incompatible.\u001b[0m\n\u001b[31mERROR: datalab 1.1.5 has requirement pandas-profiling==1.4.0, but you'll have pandas-profiling 2.6.0 which is incompatible.\u001b[0m\n\u001b[31mERROR: allennlp 0.9.0 has requirement spacy<2.2,>=2.1.0, but you'll have spacy 2.2.3 which is incompatible.\u001b[0m\nInstalling collected packages: tangled-up-in-unicode, matplotlib, phik, visions, pandas-profiling\n  Attempting uninstall: matplotlib\n    Found existing installation: matplotlib 3.1.3\n    Uninstalling matplotlib-3.1.3:\n      Successfully uninstalled matplotlib-3.1.3\n  Attempting uninstall: phik\n    Found existing installation: phik 0.9.8\n    Uninstalling phik-0.9.8:\n      Successfully uninstalled phik-0.9.8\n  Attempting uninstall: pandas-profiling\n    Found existing installation: pandas-profiling 2.4.0\n    Uninstalling pandas-profiling-2.4.0:\n      Successfully uninstalled pandas-profiling-2.4.0\nSuccessfully installed matplotlib-3.2.1 pandas-profiling-2.6.0 phik-0.9.11 tangled-up-in-unicode-0.0.4 visions-0.4.1\nRequirement already up-to-date: hypertools in /opt/conda/lib/python3.7/site-packages (0.6.2)\nRequirement already satisfied, skipping upgrade: seaborn>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from hypertools) (0.10.0)\nRequirement already satisfied, skipping upgrade: scipy>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from hypertools) (1.4.1)\nRequirement already satisfied, skipping upgrade: future in /opt/conda/lib/python3.7/site-packages (from hypertools) (0.18.2)\nRequirement already satisfied, skipping upgrade: matplotlib>=1.5.1 in /opt/conda/lib/python3.7/site-packages (from hypertools) (3.2.1)\nRequirement already satisfied, skipping upgrade: numpy>=1.10.4 in /opt/conda/lib/python3.7/site-packages (from hypertools) (1.18.1)\nRequirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.7/site-packages (from hypertools) (1.14.0)\nCollecting scikit-learn<0.22,>=0.19.1\n  Downloading scikit_learn-0.21.3-cp37-cp37m-manylinux1_x86_64.whl (6.7 MB)\n\u001b[K     |████████████████████████████████| 6.7 MB 2.6 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied, skipping upgrade: deepdish in /opt/conda/lib/python3.7/site-packages (from hypertools) (0.3.6)\nRequirement already satisfied, skipping upgrade: PPCA>=0.0.2 in /opt/conda/lib/python3.7/site-packages (from hypertools) (0.0.4)\nRequirement already satisfied, skipping upgrade: requests in /opt/conda/lib/python3.7/site-packages (from hypertools) (2.23.0)\nRequirement already satisfied, skipping upgrade: pandas>=0.18.0 in /opt/conda/lib/python3.7/site-packages (from hypertools) (1.0.1)\nRequirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=1.5.1->hypertools) (2.4.6)\nRequirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=1.5.1->hypertools) (2.8.1)\nRequirement already satisfied, skipping upgrade: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=1.5.1->hypertools) (0.10.0)\nRequirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=1.5.1->hypertools) (1.1.0)\nRequirement already satisfied, skipping upgrade: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn<0.22,>=0.19.1->hypertools) (0.14.1)\nRequirement already satisfied, skipping upgrade: tables in /opt/conda/lib/python3.7/site-packages (from deepdish->hypertools) (3.6.1)\nRequirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->hypertools) (2020.4.5.1)\nRequirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->hypertools) (1.25.7)\nRequirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->hypertools) (3.0.4)\nRequirement already satisfied, skipping upgrade: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->hypertools) (2.9)\nRequirement already satisfied, skipping upgrade: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.18.0->hypertools) (2019.3)\nRequirement already satisfied, skipping upgrade: setuptools in /opt/conda/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib>=1.5.1->hypertools) (45.2.0.post20200209)\n","name":"stdout"},{"output_type":"stream","text":"Requirement already satisfied, skipping upgrade: numexpr>=2.6.2 in /opt/conda/lib/python3.7/site-packages (from tables->deepdish->hypertools) (2.7.1)\n\u001b[31mERROR: tpot 0.11.1 has requirement scikit-learn>=0.22.0, but you'll have scikit-learn 0.21.3 which is incompatible.\u001b[0m\n\u001b[31mERROR: kmeans-smote 0.1.2 has requirement imbalanced-learn<0.5,>=0.4.0, but you'll have imbalanced-learn 0.6.2 which is incompatible.\u001b[0m\n\u001b[31mERROR: kmeans-smote 0.1.2 has requirement numpy<1.16,>=1.13, but you'll have numpy 1.18.1 which is incompatible.\u001b[0m\n\u001b[31mERROR: kmeans-smote 0.1.2 has requirement scikit-learn<0.21,>=0.19.0, but you'll have scikit-learn 0.21.3 which is incompatible.\u001b[0m\n\u001b[31mERROR: imbalanced-learn 0.6.2 has requirement scikit-learn>=0.22, but you'll have scikit-learn 0.21.3 which is incompatible.\u001b[0m\n\u001b[31mERROR: datalab 1.1.5 has requirement pandas-profiling==1.4.0, but you'll have pandas-profiling 2.6.0 which is incompatible.\u001b[0m\n\u001b[31mERROR: cesium 0.9.12 has requirement scikit-learn>=0.22.1, but you'll have scikit-learn 0.21.3 which is incompatible.\u001b[0m\n\u001b[31mERROR: allennlp 0.9.0 has requirement spacy<2.2,>=2.1.0, but you'll have spacy 2.2.3 which is incompatible.\u001b[0m\nInstalling collected packages: scikit-learn\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 0.22.2.post1\n    Uninstalling scikit-learn-0.22.2.post1:\n      Successfully uninstalled scikit-learn-0.22.2.post1\nSuccessfully installed scikit-learn-0.21.3\nCollecting pandas\n  Downloading pandas-1.0.3-cp37-cp37m-manylinux1_x86_64.whl (10.0 MB)\n\u001b[K     |████████████████████████████████| 10.0 MB 2.7 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas) (2.8.1)\nRequirement already satisfied, skipping upgrade: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from pandas) (1.18.1)\nRequirement already satisfied, skipping upgrade: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas) (2019.3)\nRequirement already satisfied, skipping upgrade: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas) (1.14.0)\n\u001b[31mERROR: tpot 0.11.1 has requirement scikit-learn>=0.22.0, but you'll have scikit-learn 0.21.3 which is incompatible.\u001b[0m\n\u001b[31mERROR: datalab 1.1.5 has requirement pandas-profiling==1.4.0, but you'll have pandas-profiling 2.6.0 which is incompatible.\u001b[0m\n\u001b[31mERROR: cesium 0.9.12 has requirement scikit-learn>=0.22.1, but you'll have scikit-learn 0.21.3 which is incompatible.\u001b[0m\nInstalling collected packages: pandas\n  Attempting uninstall: pandas\n    Found existing installation: pandas 1.0.1\n    Uninstalling pandas-1.0.1:\n      Successfully uninstalled pandas-1.0.1\nSuccessfully installed pandas-1.0.3\n","name":"stdout"}]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport re\nimport gensim\n\nimport numpy as np \nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\n\nfrom textblob import TextBlob\nfrom gensim.models.ldamodel import LdaModel\nfrom gensim.corpora.dictionary import Dictionary\n\nfrom nltk import tokenize\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom nltk.corpus import wordnet\nfrom bs4 import BeautifulSoup\nfrom gensim import matutils\n\nimport nltk\nnltk.download('wordnet')\n\nfrom sklearn.feature_extraction.text import CountVectorizer","execution_count":2,"outputs":[{"output_type":"stream","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"base_path = \"/kaggle\"\n\n# Chech if its in kaggle environment \nif os.path.exists(base_path):\n    input_path = os.path.join(base_path, \"input\", \"nlp-getting-started\")\n    output_path = os.path.join(base_path, \"working\")\nelse:\n    base_path = \"data\"\n    input_path = base_path\n    output_path = os.path.join(base_path, \"submissions\")\n\n\ntrain_file = os.path.join(input_path, \"train.csv\")\ntest_file = os.path.join(input_path, \"test.csv\")","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(train_file)\ntest_df = pd.read_csv(test_file)\n\ntrain_df.head()","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_names = [\"Not real disaster\", \"Real disaster\"]\nreal_disaster = 1","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data cleaning"},{"metadata":{},"cell_type":"markdown","source":"#### Lemmatize example"},{"metadata":{"trusted":true},"cell_type":"code","source":"WordNetLemmatizer().lemmatize(\"went\", \"v\")","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"'go'"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Stemming example"},{"metadata":{"trusted":true},"cell_type":"code","source":"stemmer = SnowballStemmer(\"english\")\nlist(map(stemmer.stem, [\"houses\", \"feets\", \"avengers\"]))","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"['hous', 'feet', 'aveng']"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"POS tagging"},{"metadata":{"trusted":true},"cell_type":"code","source":"sentence1 = [\"I\", \"played\", \"football\", \"on\", \"Anfield\", \"with\", \"Gerard\"]\nsentence2 = [\"I\", \"did\", \"not\", \"buy\", \"a\", \"cd\", \"from\", \"ACDC\"]","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(nltk.pos_tag(sentence1))\nprint(nltk.pos_tag(sentence2))","execution_count":9,"outputs":[{"output_type":"stream","text":"[('I', 'PRP'), ('played', 'VBD'), ('football', 'NN'), ('on', 'IN'), ('Anfield', 'NNP'), ('with', 'IN'), ('Gerard', 'NNP')]\n[('I', 'PRP'), ('did', 'VBD'), ('not', 'RB'), ('buy', 'VB'), ('a', 'DT'), ('cd', 'NN'), ('from', 'IN'), ('ACDC', 'NNP')]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Lemmatize + POS Tag"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_wordnet_pos(sentence):\n    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n    tag_dict = {\"J\": wordnet.ADJ,\n                \"N\": wordnet.NOUN,\n                \"V\": wordnet.VERB,\n                \"R\": wordnet.ADV}\n    \n    tagging = nltk.pos_tag(sentence)\n    result = [tag_dict.get(tag[0], wordnet.NOUN) for _, tag in tagging]\n\n    return result","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tags1 = get_wordnet_pos(sentence1)\ntags2 = get_wordnet_pos(sentence2)\n\nlemmatizer = WordNetLemmatizer()\n\nprint([lemmatizer.lemmatize(word, tag) for word, tag in zip(sentence1, tags1)])\nprint([lemmatizer.lemmatize(word, tag) for word, tag in zip(sentence2, tags2)])","execution_count":11,"outputs":[{"output_type":"stream","text":"['I', 'play', 'football', 'on', 'Anfield', 'with', 'Gerard']\n['I', 'do', 'not', 'buy', 'a', 'cd', 'from', 'ACDC']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"TextBlob Lemmatizer with POS tagging"},{"metadata":{"trusted":true},"cell_type":"code","source":"def lemmatize_with_postag(sentence):\n    sent = TextBlob(sentence)\n    tag_dict = {\"J\": 'a', \n                \"N\": 'n', \n                \"V\": 'v', \n                \"R\": 'r'}\n    words_and_tags = [(w, tag_dict.get(pos[0], 'n')) for w, pos in sent.tags]    \n    lemmatized_list = [wd.lemmatize(tag) for wd, tag in words_and_tags]\n    \n    return \" \".join(lemmatized_list)\n\n# Lemmatize\nsentence = \"The striped bats are hanging on their feet for best\"\nprint(sentence)\nlemmatize_with_postag(sentence)","execution_count":13,"outputs":[{"output_type":"stream","text":"The striped bats are hanging on their feet for best\n","name":"stdout"},{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"'The striped bat be hang on their foot for best'"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Apply this to our dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_html_encoding(text):\n    text = BeautifulSoup(text, 'lxml').get_text()\n    return text\n\n\ndef remove_html_tags(text):\n    html = re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\n\ndef remove_urls(text):\n    links_reg = re.compile(r'https?://[A-Za-z0-9./]+')\n    return links_reg.sub(r'', text)\n\n\ndef remove_mentions(text):\n    mentions_reg = re.compile(r'@[A-Za-z0-9]+')\n    return mentions_reg.sub(r'', text)\n\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n\ndef remove_puntuation(tokens):\n    return [token for token in tokens if token.isalpha()]\n\ndef clean_chars_and_contracts(tweet):\n    # Special characters\n    tweet = re.sub(r\"\\x89Û_\", \"\", tweet)\n    tweet = re.sub(r\"\\x89ÛÒ\", \"\", tweet)\n    tweet = re.sub(r\"\\x89ÛÓ\", \"\", tweet)\n    tweet = re.sub(r\"\\x89ÛÏWhen\", \"When\", tweet)\n    tweet = re.sub(r\"\\x89ÛÏ\", \"\", tweet)\n    tweet = re.sub(r\"China\\x89Ûªs\", \"China's\", tweet)\n    tweet = re.sub(r\"let\\x89Ûªs\", \"let's\", tweet)\n    tweet = re.sub(r\"\\x89Û÷\", \"\", tweet)\n    tweet = re.sub(r\"\\x89Ûª\", \"\", tweet)\n    tweet = re.sub(r\"\\x89Û\\x9d\", \"\", tweet)\n    tweet = re.sub(r\"å_\", \"\", tweet)\n    tweet = re.sub(r\"\\x89Û¢\", \"\", tweet)\n    tweet = re.sub(r\"\\x89Û¢åÊ\", \"\", tweet)\n    tweet = re.sub(r\"fromåÊwounds\", \"from wounds\", tweet)\n    tweet = re.sub(r\"åÊ\", \"\", tweet)\n    tweet = re.sub(r\"åÈ\", \"\", tweet)\n    tweet = re.sub(r\"JapÌ_n\", \"Japan\", tweet)    \n    tweet = re.sub(r\"Ì©\", \"e\", tweet)\n    tweet = re.sub(r\"å¨\", \"\", tweet)\n    tweet = re.sub(r\"SuruÌ¤\", \"Suruc\", tweet)\n    tweet = re.sub(r\"åÇ\", \"\", tweet)\n    tweet = re.sub(r\"å£3million\", \"3 million\", tweet)\n    tweet = re.sub(r\"åÀ\", \"\", tweet)\n    \n    # Contractions\n    tweet = re.sub(r\"he's\", \"he is\", tweet)\n    tweet = re.sub(r\"there's\", \"there is\", tweet)\n    tweet = re.sub(r\"We're\", \"We are\", tweet)\n    tweet = re.sub(r\"That's\", \"That is\", tweet)\n    tweet = re.sub(r\"won't\", \"will not\", tweet)\n    tweet = re.sub(r\"they're\", \"they are\", tweet)\n    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n    tweet = re.sub(r\"don\\x89Ûªt\", \"do not\", tweet)\n    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"What's\", \"What is\", tweet)\n    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n    tweet = re.sub(r\"There's\", \"There is\", tweet)\n    tweet = re.sub(r\"He's\", \"He is\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"You're\", \"You are\", tweet)\n    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n    tweet = re.sub(r\"I\\x89Ûªm\", \"I am\", tweet)\n    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n    tweet = re.sub(r\"you've\", \"you have\", tweet)\n    tweet = re.sub(r\"you\\x89Ûªve\", \"you have\", tweet)\n    tweet = re.sub(r\"we're\", \"we are\", tweet)\n    tweet = re.sub(r\"what's\", \"what is\", tweet)\n    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n    tweet = re.sub(r\"we've\", \"we have\", tweet)\n    tweet = re.sub(r\"it\\x89Ûªs\", \"it is\", tweet)\n    tweet = re.sub(r\"doesn\\x89Ûªt\", \"does not\", tweet)\n    tweet = re.sub(r\"It\\x89Ûªs\", \"It is\", tweet)\n    tweet = re.sub(r\"Here\\x89Ûªs\", \"Here is\", tweet)\n    tweet = re.sub(r\"who's\", \"who is\", tweet)\n    tweet = re.sub(r\"I\\x89Ûªve\", \"I have\", tweet)\n    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n    tweet = re.sub(r\"can\\x89Ûªt\", \"cannot\", tweet)\n    tweet = re.sub(r\"would've\", \"would have\", tweet)\n    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n    tweet = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", tweet)\n    tweet = re.sub(r\"We've\", \"We have\", tweet)\n    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n    tweet = re.sub(r\"That\\x89Ûªs\", \"That is\", tweet)\n    tweet = re.sub(r\"they've\", \"they have\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"should've\", \"should have\", tweet)\n    tweet = re.sub(r\"You\\x89Ûªre\", \"You are\", tweet)\n    tweet = re.sub(r\"where's\", \"where is\", tweet)\n    tweet = re.sub(r\"Don\\x89Ûªt\", \"Do not\", tweet)\n    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n    tweet = re.sub(r\"They're\", \"They are\", tweet)\n    tweet = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", tweet)\n    tweet = re.sub(r\"you\\x89Ûªll\", \"you will\", tweet)\n    tweet = re.sub(r\"I\\x89Ûªd\", \"I would\", tweet)\n    tweet = re.sub(r\"let's\", \"let us\", tweet)\n    tweet = re.sub(r\"it's\", \"it is\", tweet)\n    tweet = re.sub(r\"can't\", \"cannot\", tweet)\n    tweet = re.sub(r\"don't\", \"do not\", tweet)\n    tweet = re.sub(r\"you're\", \"you are\", tweet)\n    tweet = re.sub(r\"i've\", \"I have\", tweet)\n    tweet = re.sub(r\"that's\", \"that is\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n    tweet = re.sub(r\"ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"you'll\", \"you will\", tweet)\n    tweet = re.sub(r\"I've\", \"I have\", tweet)\n    tweet = re.sub(r\"Don't\", \"do not\", tweet)\n    tweet = re.sub(r\"I'll\", \"I will\", tweet)\n    tweet = re.sub(r\"I'd\", \"I would\", tweet)\n    tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n    tweet = re.sub(r\"you'd\", \"You would\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n    tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n    tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n    tweet = re.sub(r\"youve\", \"you have\", tweet)  \n    tweet = re.sub(r\"donå«t\", \"do not\", tweet)   \n\n    return tweet\n\ndef clean_text(text):\n    text = clean_html_encoding(text)\n    text = remove_html_tags(text)\n    text = remove_urls(text)\n    text = remove_mentions(text)\n    text = remove_emoji(text)\n    text = clean_chars_and_contracts(text)\n    \n    tokens = tokenize.word_tokenize(text)\n    tokens = remove_puntuation(tokens)\n\n    return \" \".join(tokens)","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.loc[:, \"text_cleaned\"] = train_df.text.apply(clean_text)\ntest_df.loc[:, \"text_cleaned\"] = test_df.text.apply(clean_text)\n\ntrain_df.loc[:, \"text_cleaned_v2\"] = train_df.text_cleaned.apply(lemmatize_with_postag)\ntest_df.loc[:, \"text_cleaned_v2\"] = test_df.text_cleaned.apply(lemmatize_with_postag)","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_df = train_df[[\"text\", \"text_cleaned\", \"text_cleaned_v2\"]].sample(n=5)\nfor text, text_cleaned, text_cleaned_v2 in zip(sample_df.text, sample_df.text_cleaned, sample_df.text_cleaned_v2):\n    print(f\"Original:\\n{text}\")\n    print(f\"Cleaned:\\n{text_cleaned}\")\n    print(f\"Lemmatize:\\n{text_cleaned_v2}\")\n    print(\"-\"*25)","execution_count":16,"outputs":[{"output_type":"stream","text":"Original:\nPhotos: 17 people killed and over 25 injured in deadly Saudi Mosque suicide attack http://t.co/geEKnwJJSz\nCleaned:\nPhotos people killed and over injured in deadly Saudi Mosque suicide attack\nLemmatize:\nPhotos people kill and over injure in deadly Saudi Mosque suicide attack\n-------------------------\nOriginal:\nUpdate: Bend FD says roofing co. workers accidentally cut through natural gas line in Post Office leading to evacuation for about a half-hr\nCleaned:\nUpdate Bend FD says roofing workers accidentally cut through natural gas line in Post Office leading to evacuation for about a\nLemmatize:\nUpdate Bend FD say roof worker accidentally cut through natural gas line in Post Office lead to evacuation for about a\n-------------------------\nOriginal:\n.@karijobe and her band killed it tonight.  It was almost loud enough to drown out the tambourine behind me..... @codycarnes @AG_USA\nCleaned:\nand her band killed it tonight It was almost loud enough to drown out the tambourine behind me\nLemmatize:\nand her band kill it tonight It be almost loud enough to drown out the tambourine behind me\n-------------------------\nOriginal:\nWhat happens to us as sexual trauma #survivors defines us as much as we agree with the perpetrators who hurt us.\nCleaned:\nWhat happens to us as sexual trauma survivors defines us as much as we agree with the perpetrators who hurt us\nLemmatize:\nWhat happen to u a sexual trauma survivor define u as much a we agree with the perpetrator who hurt u\n-------------------------\nOriginal:\n@Kiwi_Karyn Check out what's in my parking lot!! He said that until last year it was an ambulance in St Johns. http://t.co/hPvOdUD7iP\nCleaned:\nCheck out what is in my parking lot He said that until last year it was an ambulance in St Johns\nLemmatize:\nCheck out what be in my parking lot He say that until last year it be an ambulance in St Johns\n-------------------------\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = train_df[\"text_cleaned_v2\"]\n\ncv = CountVectorizer(stop_words=\"english\")\nX = cv.fit_transform(df)\n\nvocab = cv.get_feature_names()","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"common_tweets = [tweet for tweet in train_df[\"text_cleaned_v2\"].str.split()]\n\n# Create a corpus from a list of texts\ncommon_dictionary = Dictionary(common_tweets)\ncommon_corpus = [common_dictionary.doc2bow(tweet) for tweet in common_tweets]\n\n# Train the model on the corpus.\nlda = gensim.models.LdaMulticore(common_corpus, num_topics=2, passes=50, workers=4)","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for idx, topic in lda.print_topics():\n    print(\"Topic: {} \\nWords: {}\".format(idx, topic))\n    print(\"\\n\")","execution_count":24,"outputs":[{"output_type":"stream","text":"Topic: 0 \nWords: 0.011*\"9\" + 0.010*\"185\" + 0.009*\"165\" + 0.008*\"10\" + 0.008*\"26\" + 0.006*\"281\" + 0.006*\"820\" + 0.005*\"23\" + 0.005*\"35\" + 0.005*\"78\"\n\n\nTopic: 1 \nWords: 0.038*\"7\" + 0.028*\"10\" + 0.025*\"43\" + 0.020*\"35\" + 0.019*\"74\" + 0.018*\"26\" + 0.016*\"9\" + 0.014*\"75\" + 0.010*\"120\" + 0.009*\"164\"\n\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_tweets = test_df[\"text_cleaned_v2\"].str.split().values.tolist()\nother_corpus = [common_dictionary.doc2bow(text) for text in test_tweets]\n\npredictions = []\nfor oc in other_corpus:\n    vector = lda[oc]\n    y = sorted(vector, key=lambda tup: 1 - tup[1])[0][0]\n    predictions.append(y)","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_submission_file(predictions, ids, path, filename=\"submission.csv\"):\n    submission_data = {\n        \"id\": ids,\n        \"target\": predictions\n    }\n\n    submission_df = pd.DataFrame(submission_data)\n    submission_df.to_csv(os.path.join(path, filename), index=False)\n    \n    print(\"Good luck!\")","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"create_submission_file(predictions, test_df.id, output_path)","execution_count":27,"outputs":[{"output_type":"stream","text":"Good luck!\n","name":"stdout"}]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}